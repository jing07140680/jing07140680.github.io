---
date: 2024-11-02
description: "The Grand Hall"
featured_image: "" 
tags: ["scene"]
title: "New Chapter"
---

It’s been eight months since my last post — time really flies. During this period, I went through two major milestones: graduating and job hunting.

The time right after graduation was both exciting and overwhelming. I wasn’t entirely sure which direction to take — I seriously considered switching paths, perhaps diving deeper into software or data-related roles. But after rounds of reflection and interviews, I decided to stick with what I know best. I accepted a role that leans more toward electrical engineering, continuing my work in wireless communication protocols. It feels like a natural extension of what I’ve been doing, and I believe there’s still a lot of depth to explore in this field.

While I didn't end up landing a role as Machine Learning Engineer or an ML Infrastructure Software Engineer, I still believe it's important to stay up to date with AI and ML trends -- it will likely benefit me in the long run.

One of the key reasons I chose my current job is actually tied to this perspective. I believe that AI is on track to automate many entry-level and even some junior-level software roles within the next 3 to 5 years. In contrast, my current work involves domain-specific knowledge which presents a higher barrier for AI to replace -- at least for now. That said, if we eventually start contributing signifiantly to AI models in this field, we might still face a similar dilemma to what many software developers are grappling with: writing code that trains models to replace ourselves - an uncomfortable truth worth acknowledging. That day will come sooner or later, so rather than waiting passively, I need to take initiative -- investing in myself and shaping a career path that can withstand the changes ahead.


Beyond just safeguarding my career from the wave of automation brought by AI, there is also a more practical motivation for me to stay current with the lastest developments in this field. Let's be honest -- achieving true financial freedom rarely comes from salary alone (unless you are among the exceptionally talented and lucky few working deep in AI). It often stems from smart investments or bold ventures. That's why, I've started learning how cutting edge models can be turned into real, profitable products.

Admittedly, I still know very little about the inner workings of AGI or the full scope of Generative AI. But last week, I started digging deeper -- I read the AlphaEvolve paper and even traced its foundation back to the open-source prototype known as FunSearch. The basic idea behind FunSearch uses a LLM, combined with carefully crafted prompts, to generate code aimed at solving a specific mathematical problem. Once the LLM produces a piece of code, it's executed and evaluated based on predefined performance metrics. This is then used to feedback to the model in generating new and improved versions of code. This process is repeated millions of times, forming a sort of evolutionary search powered by LLM. And impressively, it has already discovered solutions better than those previously found by humands -- especially in niche mathematical or algorithmic problems that are hard to brute-force manually. I ran into some issues trying to run the official FunSearch code -- the provided scripts were incomplete. However, I came across another implementation of FunSearch that exposed more details on how to perform inference, either locally using an LLM checkpoint or via an API with carefully constructed prompts. That version made it much clearer how the whole feedback loop was structured, from code generation to evaluation and iteration.

As I experimented, I couldn't help but feel frustrated by the reality that GPUs are incredibly expensive. While I think setting up a small scale HPC lab with multiple GPUs connected via infiniBand or distributed compute isn't technically hard (if not pursuing extremely fast inference speed), it is financially prohibitive -- at least for individuals like me. This made me think something bigger: in the future, only big compannies may have the resources to conduct this kind of AI-driven research at scale. If they do discover ways to dramatically accelerate compute -- whether through better hardware or optimized algorithms -- will they share it openly with the research community? Or will it remain an internal advantage, reinforcing their dominance? It's hard not to wonder: are we heading toward a future where a few tech giants monopolize not just AI products, but also trafitionally non-tech fields -- areas like medicine, biotech, finance, etc.


I might be more conservative when it comes to many of the current AI hype cycles. One of my friends, a PhD in machine learning, holds a very different view. He sees AI as a positive force — something that will ultimately improve our world by making systems more efficient, scalable, and intelligent. In his eyes, AI isn’t just a set of tools — it’s a new way of doing things. And to be fair, I partially agree. Used thoughtfully, AI can indeed help society run better — optimizing logistics, automating tedious tasks, accelerating scientific discovery. But there’s something that still bothers me, especially when it comes to how AI systems handle edge cases and detail regressions.

A lot of AI solutions today are built around “acceptable error thresholds” — in other words, as long as the big picture looks right, it’s fine if some parts are wrong. The thinking is: “we’re getting 95% of it right, so what if the remaining 5% is a bit messy?” But I find that deeply unsettling. That 5% isn’t just “noise” — it can be critical in certain contexts. And more importantly, it reveals a kind of mindset: one that’s willing to sacrifice reliability and transparency in favor of speed or scale. Once we start tolerating this at scale — in healthcare, finance, legal decisions — we’re essentially saying: "We don't really care what happens to the outliers, as long as the metrics look good." To put it bluntly: we’re building systems that get the big picture right, while quietly saying “we don’t give a f**”* about what happens at the margins. And that, to me, is not the kind of future I want to blindly walk into.

