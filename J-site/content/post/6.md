---
date: 2024-10-06
description: "The Grand Hall"
featured_image: "" 
tags: ["scene"]
title: "Real Time ML"
---
One of my early ventures into MLOps involved working with a stock-price prediction system, which proved to be an excellent learning experience in modern ML infrastructure. The project leveraged a comprehensive tech stack: Kubernetes for orchestration, Apache Airflow for workflow management, Apache Spark for data processing, Kafka for real-time data streaming, and Cassandra for data storage. While the framework looked impressive and sophisticated, when I attempted to apply this architecture to my previous ML projects, I encountered a fundamental mismatch: this infrastructure was optimized for streaming data processing, whereas most of my projects dealt with batch machine learning tasks. And this also remind of what I thought of real-time ML. 

Looking back, I actually had encountered real-time machine learning before, during my internship at AT\&T lab. The project aimed to optimize network performance by developing an ML model that could intelligently put inactive cells into sleep mode while minimizing impact on overall network performance. This project presented several challenging problems. While incremental learning was one of them, it wasn't the primary challenge. The main obstacle lay in data collection. Since we couldn't directly use the ML model to adjust parameters in a live network, we relied on a simulator to predict system performance after parameter adjustments. The limited accuracy of this simulator and its potentially unreliable data generation might have been a key factor in the suboptimal real-world test results. My mentor's approach to incremental learning essentially involved automated retraining at regular intervals. During my brief two-month internship, I focused on several fundamental tasks: conducting basic data analysis, attempting to improve the simulator, developing better clustering methods to train specific models for each cluster, providing preliminary proof that incremental learning could enhance model performance. However, I never got the chance to fully implement the incremental learning component. 


Interestingly, after working with the stock-price prediction system later, I realized its architecture would have been perfectly suited for my AT&T project. The infrastructure design could have effectively handled the transition from batch to streaming processing that the project required.
However, I must admit that I initially struggled to fully appreciate the value of real-time machine learning. Throughout my ML journey, I had primarily worked with batch processing - collecting data points, storing them in a database, and periodically retraining models. This approach seemed sufficient for my needs.

My perspective shifted when I read Chip's example about how batch predictions can negatively impact user experience in ad ranking. This was an eye-opening moment - real-time ML wasn't just a technical novelty, but something that could directly impact business outcomes. In advertising, the difference between batch and real-time processing could mean capturing or losing a sales opportunity.

But then I wonder: does this same urgency apply to 5G networks? While faster, more responsive recommendation algorithms might lead to additional sales conversions, would more adaptive network speeds merely result in fewer complaint calls? (laughs) Perhaps this reflects a deeper issue: telecommunications providers might be too comfortable with their current profit model. When the basic business model is so reliably profitable, there's less incentive to aggressively pursue technological innovations.

(to be continuous)