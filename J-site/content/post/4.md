---
date: 2024-10-06
description: "The Grand Hall"
featured_image: "" 
tags: ["scene"]
title: "Parallel & Distributed Training/Inference of Deep Learning Models"
---
### Data parallelism
One extreme scenario for data parallelism is when the entire model fits within the GPU memory and not enough space for data.
Same model will be deployed on multiple GPUs, though the inputs are different, the output will be gathered and dispatched to each GPU to update the model.

Solution: nn.parallel.DistributedDataParallel, MPI+NCCL

### Model parallelism
  
- **Vertical Split (Layer Parallelism):**
This refers to dividing the layers of a model across multiple GPUs. In this setup, each GPU is responsible for computing a different portion of the neural network (e.g., GPU 1 processes layer 1, GPU 2 processes layer 2, and so on). This is sometimes referred to as pipeline parallelism, where data is passed sequentially through each GPU as it progresses through the layers of the model.Usually combined with batch parallelism.
  
- **Horizontal Split (Tensor Parallelism):**
This is what tensor parallelism primarily involves. Here, within each layer, the tensors (e.g., weight matrices, activations) are divided across multiple GPUs. For example, in a single matrix multiplication operation within a layer, the matrix is split into chunks, and each GPU handles part of the computation.

 
### Rematerialization of Activations

- **Forward Pass:**
	- During the forward pass, instead of saving the activations of every single layer in memory, the activations of some layers are discarded (not saved).
- **Backward Pass:**
	- When the network reaches the backward pass, it recomputes the discarded activations by running the forward pass again for the layers whose activations were not stored. These recomputed activations are then used for the backward pass to compute gradients.

Solution: checkpoint


### Offloading of Activations and Weights

Solution: checkpoint, vllm(PagedAttention)



References:

[Survey on Large Scale Neural Network Training](https://arxiv.org/pdf/2202.10435)