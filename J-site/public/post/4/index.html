<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Parallel &amp; Distributed Training/Inference of Deep Learning Models | J-site</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="The Grand Hall">
    <meta name="generator" content="Hugo 0.134.3">
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    

    
      

    

    

    
      <link rel="canonical" href="https://example.org/post/4/">
    

    <meta property="og:url" content="https://example.org/post/4/">
  <meta property="og:site_name" content="J-site">
  <meta property="og:title" content="Parallel & Distributed Training/Inference of Deep Learning Models">
  <meta property="og:description" content="The Grand Hall">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-10-06T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-10-06T00:00:00+00:00">
    <meta property="article:tag" content="Scene">

  <meta itemprop="name" content="Parallel & Distributed Training/Inference of Deep Learning Models">
  <meta itemprop="description" content="The Grand Hall">
  <meta itemprop="datePublished" content="2024-10-06T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-10-06T00:00:00+00:00">
  <meta itemprop="wordCount" content="270">
  <meta itemprop="keywords" content="Scene">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Parallel & Distributed Training/Inference of Deep Learning Models">
  <meta name="twitter:description" content="The Grand Hall">

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        J-site
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/post/" title="Articles page">
              Articles
            </a>
          </li>
          
        </ul>
      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Articles
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Parallel &amp; Distributed Training/Inference of Deep Learning Models</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2024-10-06T00:00:00Z">October 6, 2024</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h3 id="data-parallelism">Data parallelism</h3>
<p>One extreme scenario for data parallelism is when the entire model fits within the GPU memory and not enough space for data.
Same model will be deployed on multiple GPUs, though the inputs are different, the output will be gathered and dispatched to each GPU to update the model.</p>
<p>Solution: nn.parallel.DistributedDataParallel, MPI+NCCL</p>
<h3 id="model-parallelism">Model parallelism</h3>
<ul>
<li>
<p><strong>Vertical Split (Layer Parallelism):</strong>
This refers to dividing the layers of a model across multiple GPUs. In this setup, each GPU is responsible for computing a different portion of the neural network (e.g., GPU 1 processes layer 1, GPU 2 processes layer 2, and so on). This is sometimes referred to as pipeline parallelism, where data is passed sequentially through each GPU as it progresses through the layers of the model.Usually combined with batch parallelism.</p>
</li>
<li>
<p><strong>Horizontal Split (Tensor Parallelism):</strong>
This is what tensor parallelism primarily involves. Here, within each layer, the tensors (e.g., weight matrices, activations) are divided across multiple GPUs. For example, in a single matrix multiplication operation within a layer, the matrix is split into chunks, and each GPU handles part of the computation.</p>
</li>
</ul>
<h3 id="rematerialization-of-activations">Rematerialization of Activations</h3>
<ul>
<li><strong>Forward Pass:</strong>
<ul>
<li>During the forward pass, instead of saving the activations of every single layer in memory, the activations of some layers are discarded (not saved).</li>
</ul>
</li>
<li><strong>Backward Pass:</strong>
<ul>
<li>When the network reaches the backward pass, it recomputes the discarded activations by running the forward pass again for the layers whose activations were not stored. These recomputed activations are then used for the backward pass to compute gradients.</li>
</ul>
</li>
</ul>
<p>Solution: checkpoint</p>
<h3 id="offloading-of-activations-and-weights">Offloading of Activations and Weights</h3>
<p>Solution: checkpoint, vllm(PagedAttention)</p>
<p>References:</p>
<p><a href="https://arxiv.org/pdf/2202.10435">Survey on Large Scale Neural Network Training</a></p>
<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/scene/" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Scene</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/post/3/">Familiar with the foundational transformer architecture</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/2/">Scalable AI infrastructure design and implementing.</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/1/">Current main AI/ML engineer directions and corresponding requirements</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://example.org/" >
    &copy;  J-site 2025 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
